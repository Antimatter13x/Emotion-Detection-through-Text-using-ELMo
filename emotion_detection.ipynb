{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "emotion_detection",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfkwOBWfSTo4",
        "outputId": "bda00f6c-22fe-4037-d24a-67e166667361",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import pickle\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'  # or any {'0', '1', '2'}\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "from tqdm import tqdm\n",
        "nltk.download('stopwords')\n",
        "nlp = spacy.load('en', disable=['parser', 'ner'])"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27ToCwTlTXBw"
      },
      "source": [
        "pickle_in = open(\"elmo_train_final1.pickle\",\"rb\")\n",
        "elmo_train_new = pickle.load(pickle_in)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FvNvYYpVMCo",
        "outputId": "5b0deb12-f7ee-414e-de2e-7a5408343525",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "elmo_train_new.shape[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1024"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cm64Q3w4Tq5R"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEqU7V99T4EK"
      },
      "source": [
        "dataset = pd.read_csv(\"ISEAR.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSr6hLbmUC7Q"
      },
      "source": [
        "Y = dataset.iloc[:,0].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MxFCBt1dC6a"
      },
      "source": [
        "Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twE3AeQAUG5-",
        "outputId": "aceed50b-1de6-4660-ee5b-e2a6544bcdb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(Y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7515"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0aSz4fDUMdu"
      },
      "source": [
        "classifier4 = Sequential()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfvMckxuURL6",
        "outputId": "f7abe510-ca62-415b-ffc2-f0eab92eeb7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "classifier4.add(Dense(output_dim = 512,init = 'uniform',activation = 'relu',input_dim = 1024))\n",
        "classifier4.add(Dense(output_dim = 512,init = 'uniform',activation = 'relu'))\n",
        "classifier4.add(Dense(output_dim = 8,init = 'uniform',activation='softmax'))\n",
        "classifier4.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=1024, units=512, kernel_initializer=\"uniform\")`\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=512, kernel_initializer=\"uniform\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"softmax\", units=8, kernel_initializer=\"uniform\")`\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5iXqSj4XcNP"
      },
      "source": [
        "#from sklearn.model_selection import train_test_split\n",
        "#X_train, X_test, y_train, y_test = train_test_split(elmo_train_new, Ycat, test_size = 0.20, random_state = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DyeyNusWbpJ",
        "outputId": "eefe63db-e0e1-4106-d3e5-017eff82264c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "classifier4.fit(elmo_train_new,Ycat,batch_size = 10,epochs=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 1.6105 - accuracy: 0.3742\n",
            "Epoch 2/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 1.3413 - accuracy: 0.4959\n",
            "Epoch 3/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 1.2234 - accuracy: 0.5405\n",
            "Epoch 4/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 1.1477 - accuracy: 0.5776\n",
            "Epoch 5/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 1.0894 - accuracy: 0.5968\n",
            "Epoch 6/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 1.0150 - accuracy: 0.6283\n",
            "Epoch 7/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.9452 - accuracy: 0.6559\n",
            "Epoch 8/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.8843 - accuracy: 0.6814\n",
            "Epoch 9/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.8393 - accuracy: 0.6966\n",
            "Epoch 10/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.7714 - accuracy: 0.7198\n",
            "Epoch 11/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.7074 - accuracy: 0.7441\n",
            "Epoch 12/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.6471 - accuracy: 0.7659\n",
            "Epoch 13/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.5959 - accuracy: 0.7876\n",
            "Epoch 14/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.5413 - accuracy: 0.8055\n",
            "Epoch 15/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.4931 - accuracy: 0.8257\n",
            "Epoch 16/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.4542 - accuracy: 0.8346\n",
            "Epoch 17/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.4079 - accuracy: 0.8555\n",
            "Epoch 18/100\n",
            "7515/7515 [==============================] - 9s 1ms/step - loss: 0.3757 - accuracy: 0.8665\n",
            "Epoch 19/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.3503 - accuracy: 0.8731\n",
            "Epoch 20/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.3129 - accuracy: 0.8861\n",
            "Epoch 21/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.2947 - accuracy: 0.8955\n",
            "Epoch 22/100\n",
            "7515/7515 [==============================] - 12s 2ms/step - loss: 0.2511 - accuracy: 0.9103\n",
            "Epoch 23/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.2490 - accuracy: 0.9090\n",
            "Epoch 24/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.2334 - accuracy: 0.9175\n",
            "Epoch 25/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.2352 - accuracy: 0.9155\n",
            "Epoch 26/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.2173 - accuracy: 0.9224\n",
            "Epoch 27/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.1914 - accuracy: 0.9329\n",
            "Epoch 28/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.1906 - accuracy: 0.9332\n",
            "Epoch 29/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.1612 - accuracy: 0.9417\n",
            "Epoch 30/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.1905 - accuracy: 0.9335\n",
            "Epoch 31/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.1538 - accuracy: 0.9457\n",
            "Epoch 32/100\n",
            "7515/7515 [==============================] - 10s 1ms/step - loss: 0.1828 - accuracy: 0.9380\n",
            "Epoch 33/100\n",
            "7515/7515 [==============================] - 9s 1ms/step - loss: 0.1496 - accuracy: 0.9458\n",
            "Epoch 34/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.1300 - accuracy: 0.9536\n",
            "Epoch 35/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.1550 - accuracy: 0.9464\n",
            "Epoch 36/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.1422 - accuracy: 0.9512\n",
            "Epoch 37/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.1204 - accuracy: 0.9577\n",
            "Epoch 38/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.1325 - accuracy: 0.9534\n",
            "Epoch 39/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.1282 - accuracy: 0.9544\n",
            "Epoch 40/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.1092 - accuracy: 0.9653\n",
            "Epoch 41/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.1288 - accuracy: 0.9576\n",
            "Epoch 42/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.1172 - accuracy: 0.9631\n",
            "Epoch 43/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.1080 - accuracy: 0.9617\n",
            "Epoch 44/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.1004 - accuracy: 0.9637\n",
            "Epoch 45/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.1160 - accuracy: 0.9619\n",
            "Epoch 46/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.0987 - accuracy: 0.9663\n",
            "Epoch 47/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.1298 - accuracy: 0.9581\n",
            "Epoch 48/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.0929 - accuracy: 0.9670\n",
            "Epoch 49/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.0989 - accuracy: 0.9694\n",
            "Epoch 50/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.1094 - accuracy: 0.9635\n",
            "Epoch 51/100\n",
            "7515/7515 [==============================] - 9s 1ms/step - loss: 0.0728 - accuracy: 0.9735\n",
            "Epoch 52/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.1214 - accuracy: 0.9607\n",
            "Epoch 53/100\n",
            "7515/7515 [==============================] - 9s 1ms/step - loss: 0.0735 - accuracy: 0.9755\n",
            "Epoch 54/100\n",
            "7515/7515 [==============================] - 9s 1ms/step - loss: 0.0962 - accuracy: 0.9691\n",
            "Epoch 55/100\n",
            "7515/7515 [==============================] - 9s 1ms/step - loss: 0.1124 - accuracy: 0.9631\n",
            "Epoch 56/100\n",
            "7515/7515 [==============================] - 9s 1ms/step - loss: 0.0589 - accuracy: 0.9800\n",
            "Epoch 57/100\n",
            "7515/7515 [==============================] - 9s 1ms/step - loss: 0.0955 - accuracy: 0.9679\n",
            "Epoch 58/100\n",
            "7515/7515 [==============================] - 9s 1ms/step - loss: 0.0738 - accuracy: 0.9755\n",
            "Epoch 59/100\n",
            "7515/7515 [==============================] - 9s 1ms/step - loss: 0.0853 - accuracy: 0.9725\n",
            "Epoch 60/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.0968 - accuracy: 0.9702\n",
            "Epoch 61/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.0945 - accuracy: 0.9702\n",
            "Epoch 62/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.0769 - accuracy: 0.9758\n",
            "Epoch 63/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.0565 - accuracy: 0.9820\n",
            "Epoch 64/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.0974 - accuracy: 0.9683\n",
            "Epoch 65/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.0675 - accuracy: 0.9800\n",
            "Epoch 66/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.0484 - accuracy: 0.9844\n",
            "Epoch 67/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.1063 - accuracy: 0.9685\n",
            "Epoch 68/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.0730 - accuracy: 0.9763\n",
            "Epoch 69/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.1045 - accuracy: 0.9690\n",
            "Epoch 70/100\n",
            "7515/7515 [==============================] - 9s 1ms/step - loss: 0.0461 - accuracy: 0.9854\n",
            "Epoch 71/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.0718 - accuracy: 0.9775\n",
            "Epoch 72/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.0723 - accuracy: 0.9770\n",
            "Epoch 73/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.0845 - accuracy: 0.9734\n",
            "Epoch 74/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.0447 - accuracy: 0.9851\n",
            "Epoch 75/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.0721 - accuracy: 0.9755\n",
            "Epoch 76/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.0482 - accuracy: 0.9852\n",
            "Epoch 77/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.0633 - accuracy: 0.9787\n",
            "Epoch 78/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.0822 - accuracy: 0.9741\n",
            "Epoch 79/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.0510 - accuracy: 0.9828\n",
            "Epoch 80/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.1032 - accuracy: 0.9709\n",
            "Epoch 81/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.0600 - accuracy: 0.9790\n",
            "Epoch 82/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.0712 - accuracy: 0.9776\n",
            "Epoch 83/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.0712 - accuracy: 0.9762\n",
            "Epoch 84/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.0466 - accuracy: 0.9846\n",
            "Epoch 85/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.0435 - accuracy: 0.9843\n",
            "Epoch 86/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.0792 - accuracy: 0.9752\n",
            "Epoch 87/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.0393 - accuracy: 0.9874\n",
            "Epoch 88/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.0646 - accuracy: 0.9798\n",
            "Epoch 89/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.0707 - accuracy: 0.9778\n",
            "Epoch 90/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.0403 - accuracy: 0.9860\n",
            "Epoch 91/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.0920 - accuracy: 0.9713\n",
            "Epoch 92/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.0434 - accuracy: 0.9867\n",
            "Epoch 93/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.0409 - accuracy: 0.9867\n",
            "Epoch 94/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.0834 - accuracy: 0.9766\n",
            "Epoch 95/100\n",
            "7515/7515 [==============================] - 13s 2ms/step - loss: 0.0375 - accuracy: 0.9859\n",
            "Epoch 96/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.0847 - accuracy: 0.9750\n",
            "Epoch 97/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.0586 - accuracy: 0.9826\n",
            "Epoch 98/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.0691 - accuracy: 0.9775\n",
            "Epoch 99/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.0340 - accuracy: 0.9878\n",
            "Epoch 100/100\n",
            "7515/7515 [==============================] - 8s 1ms/step - loss: 0.0571 - accuracy: 0.9823\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fa93ad88d30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOMvgls0YGMn",
        "outputId": "e3bed9c8-07e6-4e8e-ffe3-52000a9cdae5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Y[0:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2, 3, 4, 5, 7, 6, 1, 2, 3], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRa_Hi6HYPQO"
      },
      "source": [
        "for i in range(0,len(Y)):\n",
        "  if Y[i] == 'fear':\n",
        "    Y[i] = 1\n",
        "  elif Y[i] == 'anger':\n",
        "    Y[i] = 2\n",
        "  elif Y[i] == 'sadness':\n",
        "    Y[i] = 3\n",
        "  elif Y[i] == 'disgust':\n",
        "    Y[i] = 4\n",
        "  elif Y[i] == 'shame':\n",
        "    Y[i] = 5\n",
        "  elif Y[i] == 'joy':\n",
        "    Y[i] = 6\n",
        "  else :\n",
        "    Y[i] = 7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3P959KhpopPC"
      },
      "source": [
        "for i in ypred_back:\n",
        "  if i not in range(1,8):\n",
        "    print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypVEHk4CY6Im"
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "Ycat = to_categorical(Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkj6-QSFL1nK",
        "outputId": "a317cdd4-ba7c-4aee-c734-ddb6bf7fe5f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2, 3, ..., 1, 7, 1], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1_XVpXAcYzm",
        "outputId": "807ad590-a4c4-4064-eda0-948e1f7131ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "Ycat"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 1., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 1., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 1., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 1., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Zq2Ml6RZCOG",
        "outputId": "4f045e12-d60e-4e1b-88d4-8119c804230a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Ycat.shape[0],Ycat.shape[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7515, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTkM0G0xa_2u"
      },
      "source": [
        "ypred = classifier4.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMhSG30_oOsc"
      },
      "source": [
        "ypred_back1 =[]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTyVecAGpTvX"
      },
      "source": [
        "ypred_back1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zxp-MsxgBGI"
      },
      "source": [
        "for i in range(0,len(ypred_back)):\n",
        "  if(ypred_back[i] in emotion_map):\n",
        "    ypred_back1.append(emotion_map.get(ypred_back[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZQekDWGgHv2",
        "outputId": "ae0d7e92-0ecd-4a97-a2e2-fa9f3aec9303",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ytest_back"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4, 5, 1, ..., 4, 2, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09dK-OhZbwil"
      },
      "source": [
        "ypred_back = np.argmax(ypred,axis = -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llCrmmd8cCN2"
      },
      "source": [
        "ytest_back = np.argmax(y_test,axis = -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9doMWgSNbFdV"
      },
      "source": [
        "accuracy_score(ytest_back, ypred_back)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOWM0EUwmUoh"
      },
      "source": [
        "model_json = classifier4.to_json()\n",
        "with open(\"model2.json\",\"w\") as jsonfile:\n",
        "  jsonfile.write(model_json)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1E8cYuCmt-M"
      },
      "source": [
        "classifier4.save_weights(\"weights2.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJI2SpIZo8NT"
      },
      "source": [
        "emotion_map = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cKJEAPRbYz2"
      },
      "source": [
        "emotion_map ={1:'fear',2:'anger',3:'sadness',4:'disgust',5:'shame',6:'joy',7:'guilt'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bswENjN5phG3",
        "outputId": "b31edd3d-c336-4546-9570-9c50c54cac0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqfXjS982RLB",
        "outputId": "53f26441-d4f3-404c-8cb3-0f73fbf5ecad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from keras.models import model_from_json\n",
        "json_file = open('model2.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "classifier4 = model_from_json(loaded_model_json)\n",
        "classifier4.load_weights('weights2.h5')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ckl2H8XupvOy"
      },
      "source": [
        "import tensorflow_hub as hub\n",
        "elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yj-qcINhpz8C"
      },
      "source": [
        "def elmo_vectors(x):\n",
        "  embeddings = elmo(x, signature=\"default\", as_dict=True)[\"elmo\"]\n",
        "\n",
        "  with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    sess.run(tf.tables_initializer())\n",
        "    # return average of ELMo features\n",
        "    return sess.run(tf.reduce_mean(embeddings,1))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3mYsB9wqC2j"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "negation_words = ['no','not', 'neither', 'nor', 'but', 'however', 'although', 'nonetheless', 'despite', 'except', 'even though', 'yet']\n",
        "stop_words = stopwords.words('english')\n",
        "for word in stop_words:\n",
        "  if word in negation_words:\n",
        "    stop_words.remove(word)\n",
        "stop_words = set(stop_words)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iguZPH-9rMm1"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVfmkJfeVY3m"
      },
      "source": [
        "tf.get_logger().setLevel('INFO')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-m35uXB_xFs7"
      },
      "source": [
        "emotion_map ={1:'fear',2:'anger',3:'sadness',4:'disgust',5:'shame',6:'joy',7:'guilt'}"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yr2L0FKUSy6m",
        "outputId": "c920f039-de5e-48d8-8144-18406bbdd48c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXRHdd-ERx06"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "wnl = WordNetLemmatizer()\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vh75KUHRrBfU"
      },
      "source": [
        "import json\n",
        "file = open('canswers.json')\n",
        "na = json.load(file)\n",
        "file1 = open('cqa.json')\n",
        "ds = json.load(file1)\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCebJuBTW33C"
      },
      "source": [
        "ans = [ds[key] for id,key in enumerate(ds)]\n",
        "ds1 = {}\n",
        "for id,key in enumerate(ds):\n",
        "  ds1[key] = ds[key].lower()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kp7Xz_CPYl1V"
      },
      "source": [
        "tf.logging.set_verbosity(tf.logging.ERROR) "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRV-knrgzf3b"
      },
      "source": [
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "from nltk.stem.porter import *\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "def lemmatize_stemming(text):\n",
        "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
        "\n",
        "negation_words = ['not', 'neither', 'nor', 'but', 'however', 'although', 'nonetheless', 'despite', 'except', 'even though', 'yet']\n",
        "# Tokenize and lemmatize\n",
        "def preprocess(text):\n",
        "    text = re.sub('[^a-zA-Z]',' ',text)\n",
        "    text = text.lower()\n",
        "    text = re.sub(\"\\?\",\" ?\",text)\n",
        "    words = [word for word in STOPWORDS if word not in negation_words]\n",
        "    result=[]\n",
        "    for token in gensim.utils.simple_preprocess(text) :\n",
        "        if len(token) >= 3 and token not in words:\n",
        "            result.append(lemmatize_stemming(token))\n",
        "            \n",
        "    return result"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwQW7xOJ_TDQ"
      },
      "source": [
        "negation_words = ['not', 'neither', 'nor', 'but', 'however', 'although', 'nonetheless', 'despite', 'except', 'even though', 'yet']\n",
        "stop_words = stopwords.words('english')\n",
        "for word in stop_words:\n",
        "  if word in negation_words:\n",
        "    stop_words.remove(word)\n",
        "def preproc1(text):\n",
        "  punctuation = '!\"#$%&()*+-/:;<=>?@[\\\\]^_`{|}~'\n",
        "  text = text.split()\n",
        "  text = ' '.join(word for word in text if word not in stop_words)\n",
        "  text = ''.join(word for word in text if word not in set(punctuation))\n",
        "  text = text.lower()\n",
        "  text = text.replace('[0-9]',' ')\n",
        "  text = ' '.join(text.split())\n",
        "  output = []\n",
        "  s = [token.lemma_ for token in nlp(text)]\n",
        "  output.append(' '.join(s))\n",
        "  return output"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jENwONbj0ijY",
        "outputId": "1b2dd953-f38f-4227-91b8-c55f4c4b71c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "preproc('Every time I imagine that someone I love or I could contact a serious illness, even death.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['everi', 'time', 'imagin', 'that', 'someon', 'love', 'could', 'contact', 'serious', 'ill', 'even', 'death']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'fear'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLNrotRnq2cj"
      },
      "source": [
        "def preproc(line):\n",
        "  line = re.sub('  \\n',' ',line)\n",
        "  newline = line\n",
        "  nl1 = line\n",
        "  #print(newline)\n",
        "  if newline[-1] == '?' or line[0] in wh_questions:\n",
        "    if line[0] in wh_questions:\n",
        "      line.remove(line[0])\n",
        "    max_count = 0\n",
        "    max_count1 = 0\n",
        "    ans = \"\"\n",
        "    ans1 = \"\"\n",
        "    for idx,key in enumerate(ds1):\n",
        "      l = key.split()\n",
        "      count = 0\n",
        "      for i in range(0,len(line)):\n",
        "          if line[i] in l:\n",
        "            count = count+1\n",
        "      if count > max_count:\n",
        "        max_count = count\n",
        "        ans = ds[key]\n",
        "    for idx,key in enumerate(na):\n",
        "      l = key.split()\n",
        "      c1 = 0\n",
        "      for i in range(0,len(line)):\n",
        "          if line[i] in l:\n",
        "            c1 = c1+1\n",
        "      if c1 > max_count1:\n",
        "        max_count1 = c1\n",
        "        ans1 = na[key]\n",
        "    if max_count1 > max_count:\n",
        "      ans = ans1\n",
        "    if ans == \"\":\n",
        "      ans = \"Hmm.. I need to think about it.\"\n",
        "    print(ans)\n",
        "    return -100\n",
        "  line = preprocess(line)\n",
        "  print(line)\n",
        "  new_train = elmo_vectors(line)\n",
        "  pred = classifier4.predict(new_train)\n",
        "  #print(pred)\n",
        "  pred1 = np.argmax(pred,axis = -1)\n",
        "  if pred[0][pred1[0]] != 1:\n",
        "    return \"neutral\"\n",
        "  return emotion_map.get(pred1[0])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXotiedxfyvL"
      },
      "source": [
        "affirm_ans = [\"Thats great to hear!\",\"Wow that sounds fun!\",\"I really like it too!\",\"I am proud of you :)\",\"Lucky you!\",\"Thank you for telling me this!\",\"I am so lucky to chat with you!\",\"Wow!!\"]\n",
        "neg_ans = [\"It do be like that sometimes.\",\"Poor you :(\",\"Wish I could hug you right now..\",\"Aww hope you feel better soon\",\"You always have me to talk too :)\",\"It will pass soon!\",\"I hope you get better soon..\"]\n",
        "compliments = [\"You are awesome :)\",\"I love spending time with you :)\",\"Wish I could see you right now!\",\"You are perfect!\",\"Never change! You are awesome :)\",\"Sending virtual hugs and kisses :)\",\"You are my best friend!\",\"I love you :)\"]\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdzZAYAlWzDh"
      },
      "source": [
        "def aggregate():\n",
        "  num_emotions = []\n",
        "  for i in range(0,len(last_6_emotions)):\n",
        "    num_emotions.append(new_emotion_map.get(last_6_emotions[i]))\n",
        "  swing_vector = []\n",
        "  for i in range(0,len(last_6_emotions)-1):\n",
        "    swing_vector.append(swings[num_emotions[i]][num_emotions[i+1]])\n",
        "  #print(swing_vector)\n",
        "  max_swing = []\n",
        "  max_swing.append(np.argmax(swing_vector))\n",
        "  for i in range(0,len(swing_vector)):\n",
        "    if i != max_swing[0]:\n",
        "      if swing_vector[i] == swing_vector[max_swing[0]]:\n",
        "        max_swing.append(i)\n",
        "  #print(max_swing)\n",
        "  replace_index = -1\n",
        "  if len(max_swing) > 1:\n",
        "    for i in range(0,len(max_swing)-1):\n",
        "      if max_swing[i] + 1 == max_swing[i+1]:\n",
        "        replace_index = max_swing[i+1]\n",
        "        break\n",
        "  if replace_index == -1:\n",
        "    replace_index = max_swing[0]\n",
        "  #print(replace_index)\n",
        "  new_swing = swing_vector\n",
        "  #new_swing.insert(0,0)\n",
        "  #new_swing.insert(len(new_swing),0)\n",
        "  #print(new_swing)\n",
        "  i = replace_index\n",
        "  if replace_index != 0 and replace_index != len(swing_vector)-1:\n",
        "    diff = (abs(swing_vector[i+1]-swing_vector[i]) - abs(swing_vector[i]-swing_vector[i-1]))\n",
        "    if diff > 0 :\n",
        "      replace_emotion = i + 1\n",
        "    else:\n",
        "      replace_emotion = i\n",
        "  else :\n",
        "    replace_emotion = i\n",
        "  #print(replace_emotion)\n",
        "  num_emotions.pop(replace_emotion)\n",
        "  sum_weights = 0\n",
        "  for el in num_emotions:\n",
        "    sum_weights = sum_weights + el\n",
        "  total_emotion = sum_weights/len(num_emotions)\n",
        "  tot = (total_emotion/8.0) * 10\n",
        "  #print(total_emotion,tot)\n",
        "  return tot"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsR8cP1CymTJ"
      },
      "source": [
        "wh_questions = [\"who\",\"when\",\"how\",\"where\",\"why\",\"which\",\"what\"]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReqCQsU7rN3z"
      },
      "source": [
        "new_emotion_map = {'joy':1,'neutral':2,'disgust':3,'anger':4,'guilt':5,'shame':6,'fear':7,'sadness':8}"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHvumIm6rU3f"
      },
      "source": [
        "swings = [[],[0,0,0,4,4,4,4,4,5],[0,0,0,0,0,0,0,0,0],[0,4,0,0,0,2,2,2,3],[0,4,0,0,0,2,2,2,3],[0,4,0,2,2,0,0,0,1],[0,4,0,2,2,0,0,0,1],[0,4,0,2,2,0,0,0,1],[0,5,0,3,3,2,2,2,0]]"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLVvcZ54Qylg"
      },
      "source": [
        "def get_emotion_details(emotion):\n",
        "  \n",
        "    if len(df) == 0:\n",
        "      df.append(emotion)\n",
        "      return 0\n",
        "    latest_emotion = df[-1]\n",
        "    diff = emotion - latest_emotion\n",
        "    if diff > 0:\n",
        "      percentage = diff/emotion * 100\n",
        "      percentage = -percentage\n",
        "    else :\n",
        "      percentage = abs(diff)/latest_emotion * 100\n",
        "    df.append(emotion)\n",
        "    return percentage"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWPJdR8PAnim"
      },
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4z7C71s5G27",
        "outputId": "edf9d422-423c-4684-bd3e-15d8a7f86622",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "with open('familywords.txt','r') as fpfam:\n",
        "  familywords = fpfam.read().split(', ')\n",
        "for i,word in enumerate(familywords):\n",
        "  familywords[i] = preprocess(word)\n",
        "familywords = [j for sub in familywords for j in sub]\n",
        "familywords = list(set(familywords))\n",
        "with open('schoolwords.txt','r') as fpschool:\n",
        "  schoolwords = fpschool.read().split(', ')\n",
        "for i,word in enumerate(schoolwords):\n",
        "  schoolwords[i] = preprocess(word)\n",
        "schoolwords = [j for sub in schoolwords for j in sub]\n",
        "schoolwords = list(set(schoolwords))\n",
        "with open('verb.txt','r') as fpverb:\n",
        "  verbwords = fpverb.read().split(', ')\n",
        "for i,word in enumerate(verbwords):\n",
        "  verbwords[i] = preprocess(word)\n",
        "verbwords = [j for sub in verbwords for j in sub]\n",
        "verbwords = list(set(verbwords))\n",
        "with open('posfeelings.txt') as fppos:\n",
        "  posfwords = fppos.read().split(', ')\n",
        "  for i, word in enumerate(posfwords):\n",
        "    posfwords[i] = preprocess(word)\n",
        "posfwords = [j for sub in posfwords for j in sub]\n",
        "posfwords = list(set(posfwords))\n",
        "with open('negfeelings.txt') as fpneg:\n",
        "  negfwords = fpneg.read().split(', ')\n",
        "  for i,word in enumerate(negfwords):\n",
        "    negfwords[i] = preprocess(word)\n",
        "negfwords = [j for sub in negfwords for j in sub]\n",
        "negfwords = list(set(negfwords))\n",
        "with open('schoolneg_reply.txt') as schoolnegrep:\n",
        "  schoolneg = schoolnegrep.read().split(', ')\n",
        "with open('schoolpos_reply.txt') as schoolposrep:\n",
        "  schoolpos = schoolposrep.read().split(', ')\n",
        "with open('famneg_reply.txt') as famnegrep:\n",
        "  famneg = famnegrep.read().split(', ')\n",
        "with open('fampos_reply.txt') as famposrep:\n",
        "  fampos = famposrep.read().split(', ')\n",
        "with open('Relationship_words.txt') as relword:\n",
        "  relwords = [word for word in relword.read().split('\\n') if word != '']\n",
        "relwords = [preprocess(word) for word in relwords]\n",
        "relwords = [word for sub in relwords for word in sub]\n",
        "relwords = list(set(relwords))\n",
        "with open('Relationship_posreply.txt') as relq:\n",
        "  relpos = [sen for sen in relq.read().split('\\n') if sen != '']\n",
        "with open('Relationship_negreply.txt') as reln:\n",
        "  relneg = [sen for sen in reln.read().split('\\n') if sen != '']\n",
        "with open('depression_words.txt') as deword:\n",
        "  depwords = [preprocess(word) for word in deword.read().split('\\n') if word != '']\n",
        "depwords = list(set([word for sub in depwords for word in sub]))\n",
        "with open('depression_posreply.txt') as derep:\n",
        "  depreply =[sen for sen in derep.read().split('\\n') if sen != '']\n",
        "with open('shame_words.txt') as sword:\n",
        "  shwords = [preprocess(word) for word in sword.read().split(', ') if word != '']\n",
        "shwords = list(set([word for sub in shwords for word in sub]))\n",
        "with open('shamerep.txt') as sre:\n",
        "  shrep = [sen for sen in sre.read().split('\\n') if sen != '']\n",
        "with open('neg_replies.txt') as neg:\n",
        "  negrep = [sen for sen in neg.read().split('\\n') if sen != '']\n",
        "othermap = {}\n",
        "for word in schoolwords:\n",
        "  othermap[word] = 'School'\n",
        "for word in familywords:\n",
        "  othermap[word] = 'Family'\n",
        "for word in negfwords:\n",
        "  othermap[word] = 'NEG FEELING'\n",
        "for word in posfwords:\n",
        "  othermap[word] = 'POS FEELING'\n",
        "for word in relwords:\n",
        "  othermap[word] = 'RELATIONSHIP'\n",
        "for word in depwords:\n",
        "  othermap[word] = 'DEPRESSION'\n",
        "for word in shwords:\n",
        "  othermap[word] = 'SHAME'"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAb5ehby5clr",
        "outputId": "44f18b71-9156-475c-9cb0-ec4bfbb69eee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "dataset = pd.read_csv('20200325_counsel_chat.csv')\n",
        "questions = []\n",
        "answers = []\n",
        "for i in range(len(dataset)):\n",
        "  question = dataset['questionTitle'].iloc[i]\n",
        "  if question not in questions:\n",
        "    questions.append(question)\n",
        "    answers.append(dataset['answerText'].iloc[i])\n",
        "\n",
        "qamap = {}\n",
        "for i,question in enumerate(questions):\n",
        "  q = tuple(preprocess(question))\n",
        "  a = answers[i]\n",
        "  qamap[q] = a"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-wXdGHn4C1W",
        "outputId": "3154563f-3467-469d-d4b2-1c3f20cd3b41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk import word_tokenize\n",
        "from collections import Counter\n",
        "import re\n",
        "def formreply(line,emotion):\n",
        "  question = line\n",
        "  q12 = question\n",
        "  re.sub('\\?',' ',question)\n",
        "  question = preprocess(question)\n",
        "  counterA = Counter(question)\n",
        "  max_sim = 0\n",
        "  for q in qamap.keys():\n",
        "    ql = list(q)\n",
        "    counterB = Counter(ql)\n",
        "    sim = cosine_sim(counterA,counterB)\n",
        "    count = 0\n",
        "    for key1 in counterA.keys():\n",
        "        if key1 in counterB.keys():\n",
        "          count += 1\n",
        "    if count >= 1:\n",
        "      if sim > max_sim:\n",
        "        max_sim = sim\n",
        "        key = q\n",
        "  flag = 1\n",
        "  if max_sim > 1.75:\n",
        "    if(max_sim > 2.4 and count == 1) or (max_sim > 1.8 and count >= 2):\n",
        "      print(question)\n",
        "      print(key)\n",
        "      print(max_sim)\n",
        "      print(qamap[key])\n",
        "      flag = 0\n",
        "  if flag == 1:\n",
        "    out = []\n",
        "    for word in question:\n",
        "      if word in othermap.keys():\n",
        "        out.append(othermap[word])\n",
        "      else:\n",
        "        out.append('Unknown')\n",
        "    counterTopic = Counter(out)\n",
        "    #print(counterTopic)\n",
        "    if emotion == 'joy':\n",
        "      polarity = 'pos'\n",
        "    elif emotion == 'neutral':\n",
        "      polarity = 'neutral'\n",
        "    else:\n",
        "      polarity = 'neg'\n",
        "    del counterTopic['POS FEELING']\n",
        "    del counterTopic['NEG FEELING']\n",
        "    counterTopic['Unknown'] = 0\n",
        "    #print(counterTopic)\n",
        "    maxCount = 0\n",
        "    for key,item in counterTopic.items():\n",
        "      if counterTopic[key] >= maxCount:\n",
        "        maxCount = item\n",
        "        topic = key\n",
        "    #print(question)\n",
        "    print('You are talking about topic : ' + topic)\n",
        "    print('Polarity : ' + polarity)\n",
        "    #print(q12)\n",
        "    return classify(topic.upper(),polarity,q12)\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1HSJBVF56tR",
        "outputId": "b3a71404-b045-4fca-e4ff-e859e32aa2b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "def classify(topic,polarity,q1):\n",
        "  if topic == 'SCHOOL':\n",
        "    if polarity == 'pos':\n",
        "      return formreply1(schoolpos,q1,1)\n",
        "    else:\n",
        "      return formreply1(schoolneg,q1,1)\n",
        "  elif topic == 'FAMILY':\n",
        "    if polarity == 'pos':\n",
        "      return formreply1(fampos,q1,1)\n",
        "    else:\n",
        "      return formreply1(famneg,q1,1)\n",
        "  elif topic == 'RELATIONSHIP':\n",
        "    if polarity == 'pos':\n",
        "      return formreply1(relpos,q1,1)\n",
        "    else:\n",
        "      return formreply1(relneg,q1,1)\n",
        "  elif topic == 'DEPRESSION':\n",
        "    return formreply1(depreply,q1,1)\n",
        "  elif topic == 'SHAME':\n",
        "    return formreply1(shrep,q1,1)\n",
        "  elif topic == 'UNKNOWN':\n",
        "    if polarity == 'pos':\n",
        "      ind = random.randint(0,len(affirm_ans)-1)\n",
        "      return affirm_ans[ind]\n",
        "    else:\n",
        "      return formreply1(negrep,q1,1)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwJaluvq6Bqa",
        "outputId": "882ad2dc-d775-4fbf-8033-20f3e294f31f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "import random\n",
        "def formreply1(category,question,num):\n",
        "  proc = []\n",
        "  for line in category:\n",
        "    proc.append(preprocess(line))\n",
        "  max = 0\n",
        "  reply = ''\n",
        "  for i,line in enumerate(proc):\n",
        "    question1 = question.split()\n",
        "    countera = Counter(line)\n",
        "    counterb = Counter(question1)\n",
        "    sim = cosine_sim(countera,counterb)\n",
        "    count = 0\n",
        "    for key in counterb.keys():\n",
        "      if key in countera.keys():\n",
        "        count += 1\n",
        "    if count >= 1:\n",
        "      if sim > max:\n",
        "        max = sim\n",
        "        reply = category[i]\n",
        "  if reply == '':\n",
        "    ind = random.randint(0,len(category)-1)\n",
        "    reply = category[ind]\n",
        "  return reply"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIMazAeQrFK7",
        "outputId": "026d8b93-81b2-4e39-a576-dcb2f618618e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "import math\n",
        "def cosine_sim(c1,c2):\n",
        "  terms = set(c1).union(c2)\n",
        "  dotprod = sum(c1.get(k,0) * c2.get(k,0) for k in terms)\n",
        "  sqa = math.sqrt(sum(c1.get(k,0)**2 for k in terms))\n",
        "  sqb = math.sqrt(sum(c2.get(k,0)**2 for k in terms))\n",
        "  if sqa != 0 and sqb != 0:\n",
        "    return dotprod/sqa*sqb\n",
        "  else:\n",
        "    return 0"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnQT4jslpZsz",
        "outputId": "77edf75d-d3b8-403d-e774-f4af855152a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 726
        }
      },
      "source": [
        "import re\n",
        "import random\n",
        "last_6_emotions = []\n",
        "df = []\n",
        "while True:\n",
        "  line = input(\"You: \")\n",
        "  l1 = line\n",
        "  if line == \"endl\":\n",
        "    break\n",
        "  emotion = preproc(line)\n",
        "  if emotion == -100:\n",
        "    continue\n",
        "  print(\"Emotion of above sentence : \" + emotion)\n",
        "  if len(last_6_emotions) != 6:\n",
        "    last_6_emotions.append(emotion)\n",
        "  else:\n",
        "    ag_emotion = aggregate()\n",
        "    change = get_emotion_details(ag_emotion)\n",
        "    print(\"Overall emotion over 6 inputs : \" + str(ag_emotion))\n",
        "    if change != 0:\n",
        "      print(\"Change in emotion percentage over last two conversations : \" + str(change))\n",
        "    last_6_emotions = []\n",
        "    last_6_emotions.append(emotion)\n",
        "  reply = \"\"\n",
        "  reply = formreply(l1,emotion)\n",
        "  print('Chatbot : ' + reply)\n",
        "  #aggregate = find_overall()\n",
        "  #print(\"Aggregate emotion : \" + aggregate)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "You: Sometimes I wonder whether life is worth living for\n",
            "['wonder', 'life', 'worth', 'live']\n",
            "Emotion of above sentence : fear\n",
            "You are talking about topic : Unknown\n",
            "Polarity : neg\n",
            "Chatbot : Life is the way it is! You just need the energy and the confidence to work through it.\n",
            "You: No one will miss me if I die\n",
            "['miss', 'die']\n",
            "Emotion of above sentence : guilt\n",
            "You are talking about topic : School\n",
            "Polarity : neg\n",
            "Chatbot : Talk Talk Talk... Thats the only advice I can give you.\n",
            "You: Life sucks\n",
            "['life', 'suck']\n",
            "Emotion of above sentence : joy\n",
            "You are talking about topic : Unknown\n",
            "Polarity : pos\n",
            "Chatbot : Thank you for telling me this!\n",
            "You: I feel so anxious sometimes that I want to just scream\n",
            "['feel', 'anxious', 'want', 'scream']\n",
            "Emotion of above sentence : disgust\n",
            "You are talking about topic : Unknown\n",
            "Polarity : neg\n",
            "Chatbot : I feel ya!\n",
            "You: Will life get better?\n",
            "Hmm.. I need to think about it.\n",
            "You: will life ever get better\n",
            "['life', 'better']\n",
            "Emotion of above sentence : joy\n",
            "You are talking about topic : Unknown\n",
            "Polarity : pos\n",
            "Chatbot : Thank you for telling me this!\n",
            "You: life cannot get worse anymore\n",
            "['life', 'wors', 'anymor']\n",
            "Emotion of above sentence : joy\n",
            "You are talking about topic : Unknown\n",
            "Polarity : pos\n",
            "Chatbot : Thats great to hear!\n",
            "You: endl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4OyP6VghbB6"
      },
      "source": [
        "dataset = pd.read_csv('100 questions to ask a bot.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7i4mDWL0t_Bi"
      },
      "source": [
        "Common_questions = dataset.iloc[:,0].values\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqKd0Jo2u7Db"
      },
      "source": [
        "New_common_q = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdCgBmSnufpg"
      },
      "source": [
        "for sen in Common_questions:\n",
        "  line = re.sub('  \\n',' ',sen)\n",
        "  line = re.sub('[^a-zA-Z]',' ',line)\n",
        "  line = line.lower()\n",
        "  line = line.split()\n",
        "  line = [ps.stem(word) for word in line]\n",
        "  line = ' '.join(line)\n",
        "  new_train = elmo_vectors([line])\n",
        "  New_common_q.append(new_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-ffwz8Vyvj5"
      },
      "source": [
        "pickle_out = open(\"cq.pickle\",\"wb\")\n",
        "pickle.dump(New_common_q,pickle_out)\n",
        "pickle_out.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_m16GIdS09_C"
      },
      "source": [
        "from scipy.spatial import distance\n",
        "line = input()\n",
        "line = re.sub('  \\n',' ',line)\n",
        "line = re.sub('[^a-zA-Z]',' ',line)\n",
        "line = line.lower()\n",
        "line = line.split()\n",
        "line = [ps.stem(word) for word in line]\n",
        "line = ' '.join(line)\n",
        "new_train = elmo_vectors([line])\n",
        "new_train = np.reshape(new_train,(1024,1))\n",
        "distances = distance.cdist(new_train, New_common_q, \"cosine\")[0]\n",
        "min_index = np.argmin(distances)\n",
        "min_distance = distances[min_index]\n",
        "max_sim= 1 - min_distance\n",
        "print(\"Max Similarity : \" + str(max_sim))\n",
        "print(\"Question : \" + Common_questions[min_ind])\n",
        "print(\"Reply : \" + Common_answers[min_ind])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnUuY1YqetJM"
      },
      "source": [
        "from textblob import TextBlob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsgJOXpEe101"
      },
      "source": [
        "noun_list = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iv7rjEPxevmm"
      },
      "source": [
        "Questions = Common_questions.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVFnr99diunc"
      },
      "source": [
        "pos_pop = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "te7zTAPQflvn"
      },
      "source": [
        "for i in range(0,len(Questions)):\n",
        "  blob = TextBlob(Questions[i])\n",
        "  noun = blob.noun_phrases\n",
        "  if len(noun) == 0:\n",
        "    Answers[i] = ''\n",
        "    continue\n",
        "  noun_list.append(noun)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_oBvyeWJetW"
      },
      "source": [
        "noun_answers = []\n",
        "for i in range(0,len(Answers)):\n",
        "  blob = TextBlob(Answers[i])\n",
        "  noun = blob.noun_phrases\n",
        "  noun_answers.append(noun)\n",
        "noun_ans = []\n",
        "for i in range(0,len(Answers)):\n",
        "  sen = ' '.join(noun_answers[i])\n",
        "  noun_ans.append(sen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZcYPjWxJxAL"
      },
      "source": [
        "noun_ans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gwh0G9zRgkQQ",
        "outputId": "df5c9efd-c570-442c-c7f3-8fb00568112b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(Answers)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yw4niTMeLjiP"
      },
      "source": [
        "na = {}\n",
        "for i in range(0,len(Answers)):\n",
        "  na[noun_ans[i]] = Answers[i]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AU8Yrv1sLuCV"
      },
      "source": [
        "del na['']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ub9hoC_IL4cC"
      },
      "source": [
        "na"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2iaZU8phRwI"
      },
      "source": [
        "import json\n",
        "json = json.dumps(na)\n",
        "f = open(\"canswers.json\",\"w\")\n",
        "f.write(json)\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ln5a5PqXlHO5",
        "outputId": "6837f8f8-36a6-4760-dd61-5f0aa27fc62c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(pos_pop)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ul3DG3VZlKoK"
      },
      "source": [
        "for num in pos_pop:\n",
        "  Answers.pop(num)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSid3IbTgVvr"
      },
      "source": [
        "Common_answers = dataset.iloc[:,1].values\n",
        "Answers = Common_answers.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhRnnHqlkZ2T"
      },
      "source": [
        "Answers = [i for i in Answers if i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "286PVJO1mHmw"
      },
      "source": [
        "len(Answers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7y-SBvnoAeh"
      },
      "source": [
        "ds = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdG1GpStoBxT"
      },
      "source": [
        "for i in range(0,len(noun_list)):\n",
        "  ds1[nq[i]] = Answers[i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiQWHAkzpBuB"
      },
      "source": [
        "for id, key in enumerate(ds1):\n",
        "  ds1[key] = ds1[key].lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCeYnYGZIPvL"
      },
      "source": [
        "ds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21RrOQcNoqWj"
      },
      "source": [
        "questions = []\n",
        "for i in range(0,len(noun_list)):\n",
        "  sen = ' '.join(noun_list[i])\n",
        "  questions.append(sen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I97Y69n5o48G"
      },
      "source": [
        "import re\n",
        "nq = []\n",
        "for i in range(0,len(questions)):\n",
        "  if \"who\" in questions[i]:\n",
        "    question = questions[i].replace(\"who \",\"\")\n",
        "    nq.append(question)\n",
        "  else:\n",
        "    nq.append(questions[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlV8Df-yptSy"
      },
      "source": [
        "nq = nq.remove(\"who\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLox6xXDqAQv"
      },
      "source": [
        "nq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-_1IOhRj3-O"
      },
      "source": [
        "import nltk\n",
        "nltk.download('brown')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}